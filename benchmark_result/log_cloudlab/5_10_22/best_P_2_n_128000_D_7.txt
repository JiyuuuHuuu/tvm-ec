Lowered TIR:
@main = primfn(A_1: handle, B_1: handle, bitmul_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {bitmul: Buffer(bitmul_2: Pointer(uint8), uint8, [16, 128000], []),
             B: Buffer(B_2: Pointer(uint8), uint8, [56, 128000], []),
             A: Buffer(A_2: Pointer(uint8), uint8, [16, 56], [])}
  buffer_map = {A_1: A, B_1: B, bitmul_1: bitmul} {
  allocate(auto_scheduler_layout_transform: Pointer(global uint8), uint8, [7168000]), storage_scope = global {
    for (ax0.ax1.fused.ax2.fused: int32, 0, 2000) "parallel" {
      for (ax4: int32, 0, 7) {
        for (ax6: int32, 0, 8) {
          for (ax7: int32, 0, 64) {
            auto_scheduler_layout_transform[((((ax0.ax1.fused.ax2.fused*3584) + (ax4*512)) + (ax6*64)) + ax7)] = (uint8*)B_2[((((ax4*1024000) + (ax6*128000)) + (ax0.ax1.fused.ax2.fused*64)) + ax7)]
          }
        }
      }
    }
    for (i.outer.outer.j.outer.outer.fused: int32, 0, 8) "parallel" {
      allocate(bitmul.local: Pointer(local uint8x64), uint8x64, [4]), storage_scope = local;
      for (i.outer.inner: int32, 0, 4) {
        for (j.outer.inner: int32, 0, 250) {
          bitmul.local[ramp(0, 1, 64)] = broadcast(0u8, 64)
          bitmul.local[ramp(64, 1, 64)] = broadcast(0u8, 64)
          bitmul.local[ramp(128, 1, 64)] = broadcast(0u8, 64)
          bitmul.local[ramp(192, 1, 64)] = broadcast(0u8, 64)
          for (k.outer: int32, 0, 7) {
            for (k.inner: int32, 0, 8) {
              let cse_var_2: int32 = (((i.outer.inner*224) + (k.outer*8)) + k.inner)
              let cse_var_1: int32 = ((((i.outer.outer.j.outer.outer.fused*896000) + (j.outer.inner*3584)) + (k.outer*512)) + (k.inner*64))
               {
                bitmul.local[ramp(0, 1, 64)] = @tir.bitwise_xor((uint8x64*)bitmul.local[ramp(0, 1, 64)], @tir.bitwise_and(broadcast((uint8*)A_2[cse_var_2], 64), (uint8x64*)auto_scheduler_layout_transform[ramp(cse_var_1, 1, 64)], dtype=uint8x64), dtype=uint8x64)
                bitmul.local[ramp(64, 1, 64)] = @tir.bitwise_xor((uint8x64*)bitmul.local[ramp(64, 1, 64)], @tir.bitwise_and(broadcast((uint8*)A_2[(cse_var_2 + 56)], 64), (uint8x64*)auto_scheduler_layout_transform[ramp(cse_var_1, 1, 64)], dtype=uint8x64), dtype=uint8x64)
                bitmul.local[ramp(128, 1, 64)] = @tir.bitwise_xor((uint8x64*)bitmul.local[ramp(128, 1, 64)], @tir.bitwise_and(broadcast((uint8*)A_2[(cse_var_2 + 112)], 64), (uint8x64*)auto_scheduler_layout_transform[ramp(cse_var_1, 1, 64)], dtype=uint8x64), dtype=uint8x64)
                bitmul.local[ramp(192, 1, 64)] = @tir.bitwise_xor((uint8x64*)bitmul.local[ramp(192, 1, 64)], @tir.bitwise_and(broadcast((uint8*)A_2[(cse_var_2 + 168)], 64), (uint8x64*)auto_scheduler_layout_transform[ramp(cse_var_1, 1, 64)], dtype=uint8x64), dtype=uint8x64)
              }
            }
          }
          for (i.inner: int32, 0, 4) {
            bitmul_2[ramp(((((i.outer.inner*512000) + (i.inner*128000)) + (i.outer.outer.j.outer.outer.fused*16000)) + (j.outer.inner*64)), 1, 64)] = (uint8x64*)bitmul.local[ramp((i.inner*64), 1, 64)]
          }
        }
      }
    }
  }
}